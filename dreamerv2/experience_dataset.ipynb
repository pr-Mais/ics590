{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "experiences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSM8K dataset\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", padding_side=\"left\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to(device)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "llm_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define experience creation in batch\n",
    "def create_experience_batch(questions, correct_answers, generated_answers):\n",
    "    batch_experiences = []\n",
    "    for question, correct_answer, generated_answer in zip(questions, correct_answers, generated_answers):\n",
    "        state = question\n",
    "        action = tokenizer.encode(generated_answer, add_special_tokens=True)\n",
    "        next_state = question + \" -> \" + generated_answer\n",
    "        reward = 1.0 if generated_answer.strip() == correct_answer.strip() else 0.0\n",
    "        batch_experiences.append((state, action, next_state, reward))\n",
    "    return batch_experiences\n",
    "\n",
    "# Generate experiences in batches\n",
    "for i in tqdm(range(0, len(gsm8k), batch_size), desc=\"Generating experiences\"):\n",
    "    batch = gsm8k.select(range(i, min(i + batch_size, len(gsm8k))))\n",
    "    questions = batch[\"question\"]\n",
    "    correct_answers = batch[\"answer\"]\n",
    "\n",
    "    # Tokenize batch of questions\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Generate answers in batch\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=50,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode generated answers and create experiences\n",
    "    generated_answers = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    experiences.extend(create_experience_batch(questions, correct_answers, generated_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Shape mismatch: q_values torch.Size([1, 87]), target_q torch.Size([1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m rewards \u001b[38;5;241m=\u001b[39m rewards\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Compute CQL loss and optimize Q-network\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcql_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_q_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    105\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[77], line 82\u001b[0m, in \u001b[0;36mcql_loss\u001b[0;34m(q_network, target_q_network, states, actions, rewards, next_states, gamma, alpha)\u001b[0m\n\u001b[1;32m     79\u001b[0m target_q \u001b[38;5;241m=\u001b[39m (rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m next_q_values)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: [batch_size]\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Ensure q_values and target_q have matching shapes\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m q_values\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m target_q\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch: q_values \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, target_q \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_q\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Calculate Bellman loss using MSE\u001b[39;00m\n\u001b[1;32m     85\u001b[0m bellman_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(q_values, target_q)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Shape mismatch: q_values torch.Size([1, 87]), target_q torch.Size([1])"
     ]
    }
   ],
   "source": [
    "class FixedLengthExperienceDataset(Dataset):\n",
    "    def __init__(self, experiences, max_length=64):\n",
    "        self.experiences = experiences\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.experiences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        state, action, next_state, reward = self.experiences[idx]\n",
    "        \n",
    "        # Tokenize state and next_state with consistent max_length and padding\n",
    "        state_ids = tokenizer(\n",
    "            state, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_length\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        next_state_ids = tokenizer(\n",
    "            next_state, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_length\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        # Ensure action and reward are in tensor format\n",
    "        action_tensor = torch.tensor(action, dtype=torch.long)  # Treat action as a list of token IDs\n",
    "        reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
    "        \n",
    "        return state_ids, action_tensor, next_state_ids, reward_tensor\n",
    "\n",
    "\n",
    "# Initialize dataset and DataLoader\n",
    "dataset = FixedLengthExperienceDataset(experiences, max_length=max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, state_dim=768, hidden_dim=256, action_dim=tokenizer.vocab_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, state_dim)\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.embedding(state).mean(dim=1)  # Mean pooling to handle sequence length\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize Q-networks and optimizer\n",
    "q_network = QNetwork(vocab_size=tokenizer.vocab_size).to(device)\n",
    "target_q_network = QNetwork(vocab_size=tokenizer.vocab_size).to(device)\n",
    "target_q_network.load_state_dict(q_network.state_dict())\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=3e-4)\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "num_epochs = 5\n",
    "\n",
    "# Conservative Q-Learning (CQL) Loss\n",
    "def cql_loss(q_network, target_q_network, states, actions, rewards, next_states, gamma=0.99, alpha=0.1):\n",
    "    # Ensure actions is a 2D tensor with shape [batch_size, 1]\n",
    "    if actions.dim() == 1:\n",
    "        actions = actions.unsqueeze(-1)  # Shape becomes [batch_size, 1]\n",
    "        \n",
    "    # Get Q-values for the chosen actions and ensure they have shape [batch_size]\n",
    "    q_values_all = q_network(states)  # Shape: [batch_size, action_dim]\n",
    "    q_values = q_values_all.gather(1, actions).squeeze(-1)  # Gather along the action dimension, then squeeze\n",
    "\n",
    "    # Get max Q-value for the next states\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_q_network(next_states).max(1)[0]  # Shape: [batch_size]\n",
    "\n",
    "    # Compute the target Q-values with the same shape as q_values\n",
    "    target_q = (rewards + gamma * next_q_values).view(-1)  # Shape: [batch_size]\n",
    "\n",
    "    # Ensure q_values and target_q have matching shapes\n",
    "    assert q_values.shape == target_q.shape, f\"Shape mismatch: q_values {q_values.shape}, target_q {target_q.shape}\"\n",
    "\n",
    "    # Calculate Bellman loss using MSE\n",
    "    bellman_loss = nn.MSELoss()(q_values, target_q)\n",
    "\n",
    "    # Calculate conservative loss\n",
    "    logsumexp_q = torch.logsumexp(q_values_all, dim=1)  # Shape: [batch_size]\n",
    "    conservative_loss = alpha * (logsumexp_q - q_values).mean()\n",
    "\n",
    "    return bellman_loss + conservative_loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for states, actions, next_states, rewards in dataloader:\n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "\n",
    "        # Compute CQL loss and optimize Q-network\n",
    "        loss = cql_loss(q_network, target_q_network, states, actions, rewards, next_states, gamma, alpha)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Periodically update target network\n",
    "    target_q_network.load_state_dict(q_network.state_dict())\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
