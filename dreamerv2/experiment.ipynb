{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mais/Documents/gh-repos/ics590_rl_project/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/mais/Documents/gh-repos/ics590_rl_project/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.distributions as dist\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(128256, 2048)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set the device to MPS if available, otherwise CPU\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", padding_side=\"left\")\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to(device)\n",
        "\n",
        "# Set padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "llm_model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate experience dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import Levenshtein\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Function to compute BLEU score reward with smoothing\n",
        "def bleu_reward(generated_answer, correct_answer):\n",
        "    reference = [correct_answer.split()]  # BLEU expects a list of references\n",
        "    candidate = generated_answer.split()\n",
        "    \n",
        "    # Use SmoothingFunction to handle cases with low n-gram overlap\n",
        "    smoothing_function = SmoothingFunction().method1  # Method 1 is a common choice for smoothing\n",
        "    \n",
        "    score = sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
        "    return score  # BLEU score with smoothing, between 0 and 1\n",
        "\n",
        "\n",
        "# Load BERT or similar model for embedding\n",
        "similarity_model = AutoModel.from_pretrained(\"google/flan-t5-base\").to(device)\n",
        "similarity_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "# Function to compute semantic similarity reward\n",
        "def semantic_similarity_reward(generated_answer, correct_answer):\n",
        "    # Tokenize and embed both answers\n",
        "    inputs_gen = similarity_tokenizer(generated_answer, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    inputs_corr = similarity_tokenizer(correct_answer, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        embedding_gen = similarity_model(**inputs_gen).last_hidden_state.mean(dim=1)  # Mean pooling\n",
        "        embedding_corr = similarity_model(**inputs_corr).last_hidden_state.mean(dim=1)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity = F.cosine_similarity(embedding_gen, embedding_corr).item()\n",
        "    return similarity  # Reward is between 0 and 1\n",
        "\n",
        "\n",
        "# Function to compute Levenshtein similarity reward\n",
        "def levenshtein_reward(generated_answer, correct_answer):\n",
        "    distance = Levenshtein.distance(generated_answer, correct_answer)\n",
        "    max_len = max(len(generated_answer), len(correct_answer))\n",
        "    reward = 1 - (distance / max_len)  # Normalized to be between 0 and 1\n",
        "    return reward\n",
        "\n",
        "# Define a helper function to structure each experience with combined rewards\n",
        "def create_experience(question, correct_answer, generated_answer):\n",
        "    state = question\n",
        "    action = generated_answer\n",
        "    next_state = question + \" -> \" + generated_answer\n",
        "\n",
        "    # Compute rewards\n",
        "    semantic_reward = semantic_similarity_reward(generated_answer, correct_answer)\n",
        "    bleu_reward_score = bleu_reward(generated_answer, correct_answer)\n",
        "    levenshtein_reward_score = levenshtein_reward(generated_answer, correct_answer)\n",
        "\n",
        "    # Combined reward (weighted average, you can adjust weights based on relevance)\n",
        "    reward = (0.4 * semantic_reward) + (0.3 * bleu_reward_score) + (0.3 * levenshtein_reward_score)\n",
        "\n",
        "    return (state, action, next_state, reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing batches:   0%|          | 0/935 [00:07<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the GSM8K dataset\n",
        "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "\n",
        "# Define a batch size\n",
        "batch_size = 8  # Adjust batch size based on available memory\n",
        "\n",
        "# Prepare a DataLoader for batching\n",
        "class GSM8KDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        return example[\"question\"], example[\"answer\"]\n",
        "\n",
        "dataset = GSM8KDataset(gsm8k)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Create a list to store experiences\n",
        "experiences = []\n",
        "\n",
        "# Process each batch of question-answer pairs\n",
        "for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
        "    questions, correct_answers = batch\n",
        "\n",
        "    # Ensure pad_token_id is set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token if not defined\n",
        "\n",
        "    # Encode batch inputs with padding and attention mask\n",
        "    inputs = tokenizer(list(questions), return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Generate answers in batch mode\n",
        "    with torch.no_grad():\n",
        "        outputs = llm_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=50,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    # Decode each generated answer and create experiences\n",
        "    generated_answers = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    for question, correct_answer, generated_answer in zip(questions, correct_answers, generated_answers):\n",
        "        experience = create_experience(question, correct_answer, generated_answer)\n",
        "        experiences.append(experience)\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
            "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? A) 16 B) 24 C) 32 D) 48 E) 64\n",
            "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did\n"
          ]
        }
      ],
      "source": [
        "print(experiences[0][0])\n",
        "print(experiences[0][1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch.distributions as dist\n",
        "\n",
        "# Set the device to MPS if available, otherwise CPU\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "# Set padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "llm_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Define the World Model with RNN layer to aggregate sequence information\n",
        "class WorldModel(nn.Module):\n",
        "    def __init__(self, llm_model, state_dim=768):\n",
        "        super(WorldModel, self).__init__()\n",
        "        self.llm_model = llm_model\n",
        "        self.projection = nn.Linear(llm_model.config.vocab_size, state_dim)  # Project vocab_size to state_dim\n",
        "        self.rnn = nn.GRU(state_dim, state_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Ensure input tensors are on the correct device\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        \n",
        "        outputs = self.llm_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # Project the logits to the state_dim for compatibility with the GRU\n",
        "        projected_logits = self.projection(outputs.logits)  # Shape: [batch_size, sequence_length, state_dim]\n",
        "        \n",
        "        # Pass the projected logits through the RNN to obtain the state\n",
        "        _, state = self.rnn(projected_logits)\n",
        "        return state.squeeze(0)  # Shape: [batch_size, state_dim]\n",
        "\n",
        "# Define the Actor and Critic Models\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc = nn.Linear(state_dim, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        return torch.softmax(self.fc(state), dim=-1)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc = nn.Linear(state_dim, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.fc(state)\n",
        "\n",
        "# Instantiate Models\n",
        "world_model = WorldModel(llm_model).to(device)\n",
        "actor = Actor(state_dim=768, action_dim=tokenizer.vocab_size).to(device)\n",
        "critic = Critic(state_dim=768).to(device)\n",
        "\n",
        "# Set up optimizers\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=3e-4)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=3e-4)\n",
        "\n",
        "def sample_action(action_probs, epsilon=1e-10):\n",
        "    # Ensure action_probs are valid probabilities by adding epsilon and normalizing\n",
        "    action_probs = torch.clamp(action_probs, min=epsilon)  # Ensure no zero values\n",
        "    action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True)  # Normalize to sum to 1\n",
        "    \n",
        "    # Sample from the action probabilities\n",
        "    action_dist = dist.Categorical(action_probs)\n",
        "    action = action_dist.sample()\n",
        "    return action\n",
        "\n",
        "\n",
        "\n",
        "# Imagined Trajectories for Sequence-to-Sequence\n",
        "def imagine_trajectories(world_model, actor, tokenizer, initial_input_ids, attention_mask, trajectory_length=5):\n",
        "    trajectories = []\n",
        "    current_input_ids = initial_input_ids.to(device)\n",
        "    current_attention_mask = attention_mask.to(device)\n",
        "\n",
        "    for _ in range(trajectory_length):\n",
        "        state = world_model(current_input_ids, current_attention_mask)\n",
        "        action_probs = actor(state)\n",
        "        action = sample_action(action_probs)\n",
        "        trajectories.append(action)\n",
        "        action = action.unsqueeze(-1)\n",
        "        current_input_ids = torch.cat([current_input_ids, action], dim=1)\n",
        "        current_attention_mask = torch.cat([current_attention_mask, torch.ones_like(action)], dim=1)\n",
        "\n",
        "    return trajectories\n",
        "\n",
        "\n",
        "# Decoding function for Seq2Seq\n",
        "def decode_trajectories(trajectories, tokenizer):\n",
        "    decoded_texts = []\n",
        "    for trajectory in trajectories:\n",
        "        trajectory_tokens = trajectory.tolist()\n",
        "        decoded_text = tokenizer.decode(trajectory_tokens, skip_special_tokens=True)\n",
        "        decoded_texts.append(decoded_text)\n",
        "    return decoded_texts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Reward Function for Seq2Seq\n",
        "def compute_reward(predicted_seq, target_seq):\n",
        "    lev_distance = levenshtein_distance(predicted_seq, target_seq)\n",
        "    max_len = max(len(target_seq), len(predicted_seq))\n",
        "    return 1.0 - (lev_distance / max_len)\n",
        "\n",
        "# Training loop with tqdm progress bars\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_progress = tqdm(gsm8k, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
        "    \n",
        "    for example in epoch_progress:\n",
        "        prompt = example[\"question\"]  # Get the math problem\n",
        "        answer = example[\"answer\"]    # Get the correct solution\n",
        "\n",
        "        # Encode prompt and move to device\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "        # Generate real trajectory based on the prompt\n",
        "        real_state = world_model(input_ids, attention_mask)\n",
        "        action_probs = actor(real_state)\n",
        "        predicted_ids = torch.argmax(action_probs, dim=-1)  # Greedy decoding for real experience\n",
        "        predicted_text = tokenizer.decode(predicted_ids.tolist(), skip_special_tokens=True)\n",
        "\n",
        "        # Calculate reward for the real trajectory\n",
        "        reward = compute_reward(predicted_text, answer)\n",
        "\n",
        "        # Imagined trajectories\n",
        "        imagined_trajectories = imagine_trajectories(world_model, actor, tokenizer, input_ids, attention_mask)\n",
        "        decoded_imagined_trajectories = decode_trajectories(imagined_trajectories, tokenizer)\n",
        "\n",
        "        # Calculate imagined rewards and use for Actor-Critic updates\n",
        "        for imagined_text in decoded_imagined_trajectories:\n",
        "            imagined_reward = compute_reward(imagined_text, answer)\n",
        "            imagined_reward_tensor = torch.tensor([imagined_reward], device=device, requires_grad=True)\n",
        "\n",
        "            # Critic update\n",
        "            critic_value = critic(real_state.detach())  # Detach real_state to prevent retaining the graph\n",
        "            critic_loss = nn.MSELoss()(critic_value, imagined_reward_tensor)\n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward(retain_graph=False)  # Ensure graph is not retained\n",
        "            critic_optimizer.step()\n",
        "\n",
        "            # Actor update\n",
        "            actor_loss = -imagined_reward_tensor.mean()  # Maximize reward by negating\n",
        "            actor_optimizer.zero_grad()\n",
        "            actor_loss.backward(retain_graph=False)  # Ensure graph is not retained\n",
        "            actor_optimizer.step()\n",
        "\n",
        "        # Update progress bar description with loss values\n",
        "        epoch_progress.set_postfix({\n",
        "            \"Reward\": reward,\n",
        "            \"Actor Loss\": actor_loss.item(),\n",
        "            \"Critic Loss\": critic_loss.item()\n",
        "        })\n",
        "\n",
        "    print(f\"Epoch {epoch+1} complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sequence(model, tokenizer, prompt, max_length=50, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generate a meaningful text sequence based on a given prompt using MPS.\n",
        "    \n",
        "    Args:\n",
        "        model: The language model used for generation (e.g., world_model or actor).\n",
        "        tokenizer: Tokenizer corresponding to the model.\n",
        "        prompt: Initial text prompt to start generation.\n",
        "        max_length: Maximum length of the generated sequence.\n",
        "        top_p: Nucleus sampling threshold for diversity.\n",
        "        \n",
        "    Returns:\n",
        "        generated_text: The complete generated text.\n",
        "    \"\"\"\n",
        "    # Set device to MPS if available, otherwise CPU\n",
        "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "    \n",
        "    # Encode the initial prompt and move to device\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    attention_mask = torch.ones_like(input_ids).to(device)\n",
        "    \n",
        "    # Start with empty generated sequence\n",
        "    generated_sequence = input_ids.clone()\n",
        "\n",
        "    # Use torch.no_grad() to save memory during inference\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Generate model outputs for the current input sequence\n",
        "            outputs = model(input_ids=generated_sequence, attention_mask=attention_mask)\n",
        "\n",
        "            # Extract the logits for the last generated token and move to CPU for processing\n",
        "            next_token_logits = outputs.logits[:, -1, :].detach().cpu()\n",
        "\n",
        "            # Apply nucleus (top-p) sampling to filter tokens for diversity\n",
        "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "            # Create a mask to remove tokens with cumulative probability above top_p\n",
        "            sorted_indices_to_keep = cumulative_probs <= top_p\n",
        "            valid_indices = sorted_indices[sorted_indices_to_keep]\n",
        "\n",
        "            # Sample from the filtered logits\n",
        "            if valid_indices.size(0) > 0:\n",
        "                # Sample only from valid indices\n",
        "                sampled_index = torch.multinomial(torch.softmax(sorted_logits[sorted_indices_to_keep], dim=-1), num_samples=1)\n",
        "                next_token = valid_indices[sampled_index].to(device)  # Move back to MPS\n",
        "            else:\n",
        "                # If no valid indices, fall back to argmax\n",
        "                next_token = torch.argmax(next_token_logits).unsqueeze(0).to(device)\n",
        "\n",
        "            # Ensure next_token has compatible dimensions\n",
        "            next_token = next_token.view(1, 1)\n",
        "\n",
        "            # Append the new token to the generated sequence\n",
        "            generated_sequence = torch.cat([generated_sequence, next_token], dim=-1)\n",
        "            attention_mask = torch.cat([attention_mask, torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "            # Stop if the end-of-sequence token is generated\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the generated sequence into text\n",
        "    generated_text = tokenizer.decode(generated_sequence[0], skip_special_tokens=True)\n",
        "    \n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a prompt and generate a sequence\n",
        "prompt = \"2+2=\"\n",
        "generated_text = generate_sequence(model=llm_model, tokenizer=tokenizer, prompt=prompt, max_length=5, top_p=0)\n",
        "\n",
        "print(\"Generated Text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}